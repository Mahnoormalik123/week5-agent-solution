# -*- coding: utf-8 -*-
"""week5_llm_updated

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kJsJQ9kEEh8fcnzzDUOlH-Go-XKSQndm
"""

!pip install -q llama-index==0.10.20 sentence-transformers transformers[torch] python-docx faiss-cpu

!pip install -q jedi

!pip install -q llama-index

from llama_index.core import VectorStoreIndex, SimpleDirectoryReader

from google.colab import files
uploaded = files.upload()   # choose your .txt or .docx file
filename = list(uploaded.keys())[0]
print("Uploaded:", filename)

# Save into data/ for LlamaIndex
os.makedirs("data", exist_ok=True)
os.replace(filename, os.path.join("data", filename))
print("Moved to data/", filename)

import glob

def read_file(path):
    if path.lower().endswith(".docx"):
        doc = docx.Document(path)
        return "\n".join([p.text for p in doc.paragraphs])
    else:
        with open(path, "r", encoding="utf-8") as f:
            return f.read()

# if multiple files in data/, concatenates them
all_texts = []
for p in glob.glob("data/*"):
    t = read_file(p)
    all_texts.append(t)
full_text = "\n\n".join(all_texts)
print("Loaded total characters:", len(full_text))

if LLMAX:
    try:
        print("Building LlamaIndex index from 'data/' (this is optional and for compliance)...")
        documents = SimpleDirectoryReader("data").load_data()
        index = VectorStoreIndex.from_documents(documents)
        print("LlamaIndex VectorStoreIndex created.")

        retriever = VectorIndexRetriever(index=index, similarity_top_k=3)
        print("LlamaIndex retriever ready (similarity_top_k=3).")
    except Exception as e:
        print("Warning: LlamaIndex indexing failed (optional). Error:", e)
else:
    print("Skipping LlamaIndex step (not available in this environment).")

def split_into_chunks(text, chunk_size=250):
    words = text.split()
    chunks = []
    for i in range(0, len(words), chunk_size):
        chunks.append(" ".join(words[i:i+chunk_size]))
    return chunks

chunks = split_into_chunks(full_text, chunk_size=250)
print("Chunks created:", len(chunks))
print("Example chunk (first 200 chars):\n", chunks[0][:200])

# Load embedder once
embedder = SentenceTransformer("all-MiniLM-L6-v2")   # fast, free

# Compute chunk embeddings (tensor)
chunk_embeddings = embedder.encode(chunks, convert_to_tensor=True)
print("Embeddings ready:", chunk_embeddings.shape)

# Text2Text generator
generator = pipeline("text2text-generation", model="google/flan-t5-base", device_map="auto" if torch.cuda.is_available() else None)

# If device_map fails or memory is low, fall back to CPU run:
if not torch.cuda.is_available():
    generator = pipeline("text2text-generation", model="google/flan-t5-base")
print("Generator ready.")

from sentence_transformers import util
import torch

def ask_ai(query, top_k=3, max_context_len=400):
    # 1) Embed query
    q_emb = embedder.encode(query, convert_to_tensor=True)

    # 2) cosine similarity
    scores = util.pytorch_cos_sim(q_emb, chunk_embeddings)[0]
    topk = torch.topk(scores, k=min(top_k, len(chunks)))
    indices = topk.indices.tolist()
    sims = topk.values.tolist()

    # 3) Build context from top chunks
    selected_chunks = []
    for i, (idx, s) in enumerate(zip(indices, sims)):
        chunk_text = chunks[idx]
        if len(chunk_text) > max_context_len:
            chunk_text = chunk_text[:max_context_len] + "..."  # cut long chunks
        selected_chunks.append(f"Source {i+1} (score={s:.3f}):\n{chunk_text}")
    context = "\n\n".join(selected_chunks)

    # 4) Create prompt
    prompt = f"""You are an assistant. Use the following CONTEXT from the documents to answer the question simply.

CONTEXT:
{context}

QUESTION:
{query}

Answer concisely (1-4 sentences). If the answer is not in the context, say "I could not find this in the documents."
"""

    # 5) Use a summarization-friendly model
    generator = pipeline("text2text-generation", model="google/flan-t5-base")

    # 6) Generate output
    out = generator(prompt, max_new_tokens=256)
    answer = out[0]['generated_text'].strip()

    return answer

# Quick test
print(ask_ai("What is gradient descent?"))

import ast

def calculate_expr(expr):
    # safe evaluator for basic math only
    node = ast.parse(expr, mode='eval')
    def eval_node(n):
        if isinstance(n, ast.Constant):     # Python 3.8+
            return n.value
        if isinstance(n, ast.Num):          # older versions
            return n.n
        if isinstance(n, ast.BinOp):
            left = eval_node(n.left); right = eval_node(n.right)
            if isinstance(n.op, ast.Add): return left + right
            if isinstance(n.op, ast.Sub): return left - right
            if isinstance(n.op, ast.Mult): return left * right
            if isinstance(n.op, ast.Div): return left / right
            if isinstance(n.op, ast.Pow): return left ** right
        if isinstance(n, ast.UnaryOp):
            if isinstance(n.op, ast.USub): return -eval_node(n.operand)
        raise ValueError("Unsupported expression")
    return eval_node(node.body)

# combined helper
def process_query(query):
    # if the query seems arithmetic, run calculator
    if any(op in query for op in ["+", "-", "*", "/", "^", "calculate", "times", "divide"]):
        try:
            # try to extract expression (simple)
            # if query contains text, keep digits and operators only
            expr = "".join(ch for ch in query if ch.isdigit() or ch in "+-*/^.() ")
            return f"Calculator: {calculate_expr(expr)}"
        except Exception as e:
            return f"Calculator Error: {e}"
    else:
        return ask_ai(query)

# Try:
print(process_query("23*45"))
print(process_query("Explain neural networks"))

def ask_with_sources(q):
    # return answer + the source chunk texts used
    q_emb = embedder.encode(q, convert_to_tensor=True)
    scores = util.pytorch_cos_sim(q_emb, chunk_embeddings)[0]
    topk = torch.topk(scores, k=min(3,len(chunks)))
    idxs = topk.indices.tolist()
    sims = topk.values.tolist()
    ctxs = [chunks[i] for i in idxs]
    ans = ask_ai(q, top_k=3)
    return {"answer": ans, "sources": [{"index": idx, "score": float(s), "text": chunks[idx][:400]} for idx,s in zip(idxs,sims)]}

res = ask_with_sources("List applications of AI")
print("ANSWER:\n", res["answer"])
print("\nSOURCES:")
for s in res["sources"]:
    print("-", s["index"], "score", round(s["score"],3))
    print(s["text"][:200], "...\n")

# create simple src/ files for GitHub repo
os.makedirs("src/tools", exist_ok=True)
os.makedirs("docs", exist_ok=True)
with open("src/main.py","w") as f:
    f.write("# main.py: entry point (simplified)\nprint('Run the Colab notebook to use the system.')\n")
with open("src/agent.py","w") as f:
    f.write("# agent.py placeholder: In Colab we use ask_ai() defined in notebook\n")
with open("src/tools/custom_tool.py","w") as f:
    f.write("# custom_tool.py: simple calculator tool\nfrom ast import parse\n")
print("Created simple src/ placeholders.")