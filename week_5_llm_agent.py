# -*- coding: utf-8 -*-
"""WEEK_5_LLM_Agent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EExK8vgseioz6rTd2f0oPgdj21EeypQg
"""

!pip install sentence-transformers transformers python-docx

import torch
from sentence_transformers import SentenceTransformer, util
from transformers import pipeline
import docx
import ast

from google.colab import files

uploaded = files.upload()
filename = list(uploaded.keys())[0]

if filename.endswith(".docx"):
    doc = docx.Document(filename)
    text_data = "\n".join([p.text for p in doc.paragraphs])
else:
    with open(filename, "r", encoding="utf-8") as f:
        text_data = f.read()

print("File Loaded")

def split_into_chunks(text, max_len=200):
    words = text.split()
    chunks, current = [], []
    count = 0
    for w in words:
        current.append(w)
        count += 1
        if count >= max_len:
            chunks.append(" ".join(current))
            current, count = [], 0
    if current:
        chunks.append(" ".join(current))
    return chunks

chunks = split_into_chunks(text_data)
print("Total Chunks:", len(chunks))

embedder = SentenceTransformer("all-MiniLM-L6-v2")
chunk_embeddings = embedder.encode(chunks, convert_to_tensor=True)

class RetrievalAgent:
    def __init__(self, chunks):
        self.chunks = chunks
        self.embeddings = embedder.encode(chunks, convert_to_tensor=True)

    def process_query(self, query):
        query_embedding = embedder.encode(query, convert_to_tensor=True)
        scores = util.pytorch_cos_sim(query_embedding, self.embeddings)[0]
        best_idx = int(torch.argmax(scores))
        context = self.chunks[best_idx]

        generator = pipeline("text-generation", model="gpt2")
        prompt = f"""
You are an AI tutor. Use this context to answer clearly and in simple words.

Context:
{context}

Question: {query}
Answer:
"""
        result = generator(prompt, max_new_tokens=120)[0]['generated_text']
        return result

agent = RetrievalAgent(chunks)
print(" Agent Ready")

def calculate(expr):
    node = ast.parse(expr, mode='eval')

    def eval_node(node):
        if isinstance(node, ast.Constant):
            return node.value
        elif isinstance(node, ast.BinOp):
            left, right = eval_node(node.left), eval_node(node.right)
            if isinstance(node.op, ast.Add): return left + right
            if isinstance(node.op, ast.Sub): return left - right
            if isinstance(node.op, ast.Mult): return left * right
            if isinstance(node.op, ast.Div): return left / right
        else:
            raise ValueError("Unsupported operation")
    return eval_node(node.body)

def ask_ai(query):
    return agent.process_query(query)

print("Calculator:", calculate("150/6"))
print("AI Answer:", ask_ai("Explain gradient descent"))
print("AI Answer:", ask_ai("List applications of AI"))
print("AI Answer:", ask_ai("What are the strengths and weaknesses of AI?"))
print("AI Answer:", ask_ai("Give me a short summary of the report"))